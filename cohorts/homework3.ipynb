{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d172ab",
   "metadata": {},
   "source": [
    "## Homework: Search Evaluation\n",
    "\n",
    "In this homework, we will evaluate the results of vector\n",
    "search.\n",
    "\n",
    "> It's possible that your answers won't match exactly. If it's the case, select the closest one.\n",
    "\n",
    "\n",
    "## Required libraries\n",
    "\n",
    "We will use minsearch and Qdrant. Make sure you have the most up-to-date versions:\n",
    "\n",
    "```bash\n",
    "pip install -U minsearch qdrant_client\n",
    "``` \n",
    "\n",
    "minsearch should be at least 0.0.4.\n",
    "\n",
    "\n",
    "\n",
    "## Evaluation data\n",
    "\n",
    "For this homework, we will use the same dataset we generated\n",
    "in the videos.\n",
    "\n",
    "Let's get them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a34c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
    "docs_url = url_prefix + 'search_evaluation/documents-with-ids.json'\n",
    "documents = requests.get(docs_url).json()\n",
    "\n",
    "ground_truth_url = url_prefix + 'search_evaluation/ground-truth-data.csv'\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51453e28",
   "metadata": {},
   "source": [
    "Here, `documents` contains the documents from the FAQ database\n",
    "with unique IDs, and `ground_truth` contains generated\n",
    "question-answer pairs. \n",
    "\n",
    "Also, we will need the code for evaluating retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e2f884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['document']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e4792",
   "metadata": {},
   "source": [
    "## Q1. Minsearch text\n",
    "\n",
    "Now let's evaluate our usual minsearch approach, but tweak\n",
    "the parameters. Let's use the following boosting \n",
    "params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e3f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = {'question': 1.5, 'section': 0.1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102ff52",
   "metadata": {},
   "source": [
    "What's the hitrate for this approach?\n",
    "\n",
    "* 0.64\n",
    "* 0.74\n",
    "* 0.84\n",
    "* 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca23581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x72ec4871fa10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import minsearch\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\", \"id\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa92840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search(query, course):\n",
    "    boost = {'question': 1.5, 'section': 0.1}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': course},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21b6095c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9782db2025eb4ac4b8533baa19821a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = minsearch_search(query=q['question'], course=q['course'])\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0b3920e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.848714069591528"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate(relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477e9fd6",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Q1 ANSWER\n",
    "c) 0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a490dd",
   "metadata": {},
   "source": [
    "## Embeddings \n",
    "\n",
    "The latest version of minsearch also supports vector search. \n",
    "We will use it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f199cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import VectorSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eccabb",
   "metadata": {},
   "source": [
    "We will also use TF-IDF and Singular Value Decomposition to \n",
    "create embeddings from texts. You can refer to our\n",
    "[\"Create Your Own Search Engine\" workshop](https://github.com/alexeygrigorev/build-your-own-search-engine)\n",
    "if you want to know more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72fcc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab957e",
   "metadata": {},
   "source": [
    "Let's create embeddings for the \"question\" field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9956bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "for doc in documents:\n",
    "    t = doc['question']\n",
    "    texts.append(t)\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44ca6083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Course - When will the course start?',\n",
       " 'Course - What are the prerequisites for this course?',\n",
       " 'Course - Can I still join the course after the start date?',\n",
       " 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?',\n",
       " 'Course - What can I do before the course starts?',\n",
       " 'Course - how many Zoomcamps in a year?',\n",
       " 'Course - Is the current cohort going to be different from the previous cohort?',\n",
       " 'Course - Can I follow the course after it finishes?',\n",
       " 'Course - Can I get support if I take the course in the self-paced mode?',\n",
       " 'Course - Which playlist on YouTube should I refer to?',\n",
       " 'Course - \\u200b\\u200bHow many hours per week am I expected to spend on this  course?',\n",
       " 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       " 'Office Hours - What is the video/zoom link to the stream for the “Office Hour” or workshop sessions?',\n",
       " 'Office Hours - I can’t attend the “Office hours” / workshop, will it be recorded?',\n",
       " 'Homework - What are homework and project deadlines?',\n",
       " 'Homework - Are late submissions of homework allowed?',\n",
       " 'Homework - What is the homework URL in the homework link?',\n",
       " 'Homework and Leaderboard - what is the system for points in the course management platform?',\n",
       " 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?',\n",
       " 'Environment - Is Python 3.9 still the recommended version to use in 2024?',\n",
       " 'Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?',\n",
       " 'Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?',\n",
       " 'Environment - Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed.',\n",
       " 'Environment - Do I need both GitHub Codespaces and GCP?',\n",
       " 'This happens when attempting to connect to a GCP VM using VSCode on a Windows machine. Changing registry value in registry editor',\n",
       " 'Environment - Why are we using GCP and not other cloud providers?',\n",
       " 'Should I pay for cloud services?',\n",
       " 'Environment - The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?',\n",
       " 'Environment - I want to use AWS. May I do that?',\n",
       " 'Besides the “Office Hour” which are the live zoom calls?',\n",
       " 'Are we still using the NYC Trip data for January 2021? Or are we using the 2022 data?',\n",
       " 'Is the 2022 repo deleted?',\n",
       " 'Can I use Airflow instead for my final project?',\n",
       " 'Is it possible to use tool “X” instead of the one tool you use in the course?',\n",
       " 'How can we contribute to the course?',\n",
       " 'Environment - Is the course [Windows/mac/Linux/...] friendly?',\n",
       " 'Environment - Roadblock for Windows users in modules with *.sh (shell scripts).',\n",
       " 'Any books or additional resources you recommend?',\n",
       " 'Project - What is Project Attemp #1 and Project Attempt #2 exactly?',\n",
       " 'How to troubleshoot issues',\n",
       " 'How to ask questions',\n",
       " 'How do I use Git / GitHub for this course?',\n",
       " 'VS Code: Tab using spaces',\n",
       " 'Opening an HTML file with a Windows browser from Linux running on WSL',\n",
       " 'Set up Chrome Remote Desktop for Linux on Compute Engine',\n",
       " 'Taxi Data - How to handle taxi data files, now that the files are available as *.csv.gz?',\n",
       " 'Taxi Data - Data Dictionary for NY Taxi data?',\n",
       " 'Taxi Data - Unzip Parquet file',\n",
       " 'lwget is not recognized as an internal or external command',\n",
       " 'wget - ERROR: cannot verify <website> certificate  (MacOS)',\n",
       " 'Git Bash - Backslash as an escape character in Git Bash for Windows',\n",
       " 'GitHub Codespaces - How to store secrets',\n",
       " 'Docker - Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?',\n",
       " 'Docker - Error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post: \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\" : open //./pipe/docker_engine: The system cannot find the file specified',\n",
       " 'Docker - docker pull dbpage',\n",
       " 'Docker - can’t delete local folder that mounted to docker volume',\n",
       " \"Docker - Docker won't start or is stuck in settings (Windows 10 / 11)\",\n",
       " 'Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?',\n",
       " 'Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).',\n",
       " 'Docker - The input device is not a TTY (Docker run for Windows)',\n",
       " 'Docker - Cannot pip install on Docker container (Windows)',\n",
       " 'Docker - ny_taxi_postgres_data is empty',\n",
       " 'dasDocker - Setting up Docker on Mac',\n",
       " '1Docker - Could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted',\n",
       " 'Docker - invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows)',\n",
       " 'Docker - Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.',\n",
       " \"Docker - Error response from daemon: error while creating buildmount source path '/run/desktop/mnt/host/c/<your path>': mkdir /run/desktop/mnt/host/c: file exists\",\n",
       " \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\",\n",
       " 'Docker - ERRO[0000] error waiting for container: context canceled',\n",
       " 'Docker - build error checking context: can’t stat ‘/home/fhrzn/Projects/…./ny_taxi_postgres_data’',\n",
       " 'Docker - failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.',\n",
       " 'Docker - Docker network name',\n",
       " 'Docker - Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container “xxx”.  You have to remove (or rename) that container to be able to reuse that name.',\n",
       " 'Docker - ingestion when using docker-compose could not translate host name',\n",
       " 'Docker - Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).',\n",
       " 'Docker - Connecting from VS Code',\n",
       " 'Docker - PostgreSQL Database directory appears to contain a database. Database system is shut down',\n",
       " 'Docker not installable on Ubuntu',\n",
       " 'Docker-Compose - mounting error',\n",
       " 'Docker-Compose - Error translating host name to address',\n",
       " 'Docker-Compose -  Data retention (could not translate host name \"pg-database\" to address: Name or service not known)',\n",
       " 'Docker-Compose - Hostname does not resolve',\n",
       " 'Docker-Compose - Persist PGAdmin docker contents on GCP',\n",
       " 'Docker engine stopped_failed to fetch extensions',\n",
       " 'Docker-Compose - Persist PGAdmin configuration',\n",
       " 'Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied',\n",
       " 'Docker-Compose - docker-compose still not available after changing .bashrc',\n",
       " 'Docker-Compose - Error getting credentials after running docker-compose up -d',\n",
       " 'Docker-Compose - Errors pertaining to docker-compose.yml and pgadmin setup',\n",
       " 'Docker Compose up -d error getting credentials - err: exec: \"docker-credential-desktop\": executable file not found in %PATH%, out: ``',\n",
       " 'Docker-Compose - Which docker-compose binary to use for WSL?',\n",
       " 'Docker-Compose - Error undefined volume in Windows/WSL',\n",
       " 'WSL Docker directory permissions error',\n",
       " 'Docker - If pgadmin is not working for Querying in Postgres Use PSQL',\n",
       " 'WSL - Insufficient system resources exist to complete the requested service.',\n",
       " 'WSL - WSL integration with distro Ubuntu unexpectedly stopped with exit code 1.',\n",
       " 'WSL - Permissions too open at Windows',\n",
       " 'WSL - Could not resolve host name',\n",
       " 'PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused',\n",
       " 'PGCLI --help error',\n",
       " 'PGCLI - INKhould we run pgcli inside another docker container?',\n",
       " 'PGCLI - FATAL: password authentication failed for user \"root\" (You already have Postgres)',\n",
       " \"PGCLI - PermissionError: [Errno 13] Permission denied: '/some/path/.config/pgcli'\",\n",
       " 'PGCLI - no pq wrapper available.',\n",
       " 'PGCLI -  stuck on password prompt',\n",
       " 'PGCLI - pgcli: command not found',\n",
       " 'PGCLI - running in a Docker container',\n",
       " 'PGCLI - case sensitive use “Quotations” around columns with capital letters',\n",
       " 'PGCLI - error column c.relhasoids does not exist',\n",
       " 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  password authentication failed for user \"root\"',\n",
       " 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"root\" does not exist',\n",
       " 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  dodatabase \"ny_taxi\" does not exist',\n",
       " \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       " 'Postgres - \"Column does not exist\" but it actually does (Pyscopg2 error in MacBook Pro M2)',\n",
       " 'pgAdmin - Create server dialog does not appear',\n",
       " 'pgAdmin - Blank/white screen after login (browser)',\n",
       " 'pgAdmin - Can not access/open the PgAdmin address via browser',\n",
       " 'Python - Ingestion with Jupyter notebook - missing 100000 records',\n",
       " 'Python - Iteration csv without error',\n",
       " 'iPython - Pandas parsing dates with ‘read_csv’',\n",
       " 'Python - Python cant ingest data from the github link provided using curl',\n",
       " 'Python - Pandas can read *.csv.gzip',\n",
       " 'Python - How to iterate through and ingest parquet file',\n",
       " \"Python - SQLAlchemy - ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'.\",\n",
       " \"Python - SQLALchemy - TypeError 'module' object is not callable\",\n",
       " \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
       " 'GCP - Unable to add Google Cloud SDK PATH to Windows',\n",
       " 'GCP - Project creation failed: HttpError accessing … Requested entity alreadytpep_pickup_datetime exists',\n",
       " 'GCP - The project to be billed is associated with an absent billing account',\n",
       " 'GCP - OR-CBAT-15 ERROR Google cloud free trial account',\n",
       " 'GCP - Where can I find the “ny-rides.json” file?',\n",
       " 'GCP - Do I need to delete my instance in Google Cloud?',\n",
       " 'Commands to inspect the health of your VM:',\n",
       " 'Billing account has not been enabled for this project. But you’ve done it indeed!',\n",
       " 'GCP - Windows Google Cloud SDK install issue:gcp',\n",
       " 'GCP VM - Is it necessary to use a GCP VM? When is it useful?',\n",
       " 'GCP VM - mkdir: cannot create directory ‘.ssh’: Permission denied',\n",
       " 'GCP VM - Error while saving the file in VM via VS Code',\n",
       " '. GCP VM - VM connection request timeout',\n",
       " 'GCP VM -  connect to host port 22 no route to host',\n",
       " 'GCP VM - Port forwarding from GCP without using VS Code',\n",
       " 'GCP gcloud + MS VS Code - gcloud auth hangs',\n",
       " 'Terraform - Error: Failed to query available provider packages │ Could not retrieve the list of available versions for provider hashicorp/google: could not query │ provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, │ please try again later',\n",
       " 'Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout',\n",
       " 'Terraform - Install for WSL',\n",
       " 'Terraform - Error acquiring the state lock',\n",
       " 'Terraform - Error 400 Bad Request.  Invalid JWT Token  on WSL.',\n",
       " 'Terraform - Error 403 : Access denied',\n",
       " 'Terraform - Do I need to make another service account for terraform before I get the keys (.json file)?',\n",
       " 'Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?',\n",
       " 'Terraform - Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.g',\n",
       " 'Terraform - Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes',\n",
       " 'Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’',\n",
       " 'To ensure the sensitivity of the credentials file, I had to spend lot of time to input that as a file.',\n",
       " \"SQL - SELECT * FROM zones_taxi WHERE Zone='Astoria Zone'; Error Column Zone doesn't exist\",\n",
       " \"SQL - SELECT Zone FROM taxi_zones Error Column Zone doesn't exist\",\n",
       " 'CURL - curl: (6) Could not resolve host: output.csv',\n",
       " 'SSH Error: ssh: Could not resolve hostname linux: Name or service not known',\n",
       " \"'pip' is not recognized as an internal or external command, operable program or batch file.\",\n",
       " 'Error: error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use',\n",
       " 'Anaconda to PIP',\n",
       " 'Where are the FAQ questions from the previous cohorts for the orchestration module?',\n",
       " 'Docker - 2.2.2 Configure Mage',\n",
       " 'WSL - 2.2.3 Mage - Unexpected Kernel Restarts; Kernel Running out of memory:',\n",
       " '2.2.3 Configuring Postgres',\n",
       " 'MAGE - 2.2.3 OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5431 failed: Connection refused',\n",
       " 'MAGE - 2.2.4 executing SELECT 1; results in KeyError',\n",
       " \"MAGE -2.2.4 ConnectionError: ('Connection aborted.', TimeoutError('The write operation timed out'))\",\n",
       " \"Problem: RefreshError: ('invalid_grant: Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.', {'error': 'invalid_grant', 'error_description': 'Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.'})\",\n",
       " 'Mage - 2.2.4 IndexError: list index out of range',\n",
       " '2.2.6 OSError: Cannot save file into a non-existent directory: \\'..\\\\\\\\..\\\\\\\\data\\\\\\\\yellow\\'\\\\n\")',\n",
       " 'GCP - 2.2.7d Deploying Mage to GCP',\n",
       " 'Ruuning Multiple Mage instances in Docker from different directories',\n",
       " 'GCP - 2.2.7d Load Balancer Problem (Security Policies quota)',\n",
       " 'GCP - 2.2.7d Part 2 - Getting error when you run terraform apply',\n",
       " \"Question: Permission 'vpcaccess.connectors.create'\",\n",
       " \"File Path: Cannot save file into a non-existent directory: 'data/green'\",\n",
       " 'No column name lpep_pickup_datetime / tpep_pickup_datetime',\n",
       " 'Process to download the VSC using Pandas is killed right away',\n",
       " 'Push to docker image failure',\n",
       " 'Flow script fails with “killed” message:',\n",
       " 'GCP VM: Disk Space is full',\n",
       " 'Docker: container crashed with status code 137.',\n",
       " 'Timeout due to slow upload internet',\n",
       " 'UndefinedColumn: column \"ratecode_id\", \"rate_code_id\" “vendor_id”, “pu_location_id”, “do_location_id” of relation \"green_taxi\" does not exist - Export transformed green_taxi data to PostgreSQL',\n",
       " 'Homework - Q3 SettingWithCopyWarning Error:',\n",
       " 'Since I was using slow laptop, and we have so big csv files, I used pyspark kernel in mage instead of python, How to do it?',\n",
       " 'I got an error when I was deleting  BLOCK IN A PIPELINE',\n",
       " 'Mage UI won’t let you edit the Pipeline name?',\n",
       " 'How do I make Mage load the partitioned files that we created on 2.2.4, to load them into BigQuery ?',\n",
       " 'Git - What Files Should I Submit for Homework 2 & How do I get them out of MAGE:',\n",
       " 'Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?',\n",
       " 'Got ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()',\n",
       " 'Mage AI Files are Gone/disappearing',\n",
       " 'Mage - Errors in io.config.yaml file',\n",
       " 'Mage - ArrowException Cannot open credentials file',\n",
       " 'Mage - OSError',\n",
       " 'Mage - PermissionError service account does not have storage.buckets.get access to the Google Cloud Storage bucket',\n",
       " 'Trigger Dataproc from Mage',\n",
       " 'Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets',\n",
       " 'GCS Bucket - error when writing data from web to GCS:',\n",
       " \"GCS Bucket - Failed to create table: Error while reading data, error message: Parquet column 'XYZ' has type INT which does not match the target cpp_type DOUBLE. File: gs://path/to/some/blob.parquet\",\n",
       " 'GCS Bucket - Fix Error when importing FHV data to GCS',\n",
       " 'GCS Bucket - Load Data From URL list in to GCP Bucket',\n",
       " 'GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?',\n",
       " 'GCP BQ - “bq: command not found”',\n",
       " 'GCP BQ - Caution in using bigquery:no',\n",
       " 'GCP BQ - Cannot read and write in different locations: source: EU, destination: US - Loading data from GCS into BigQuery (different Region):',\n",
       " 'GCP BQ - Cannot read and write in different locations: source: <REGION_HERE>, destination: <ANOTHER_REGION_HERE>',\n",
       " 'GCP BQ - Remember to save your queries',\n",
       " 'GCP BQ - Can I use BigQuery for real-time analytics in this project?',\n",
       " 'GCP BQ - Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage',\n",
       " 'GCP BQ - Error Message in BigQuery: annotated as a valid Timestamp, please annotate it as TimestampType(MICROS) or TimestampType(MILLIS)',\n",
       " 'GCP BQ - Datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery',\n",
       " 'GCP BQ - Create External Table using Python',\n",
       " 'GCP BQ - Check BigQuery Table Exist And Delete',\n",
       " 'GCP BQ - Error: Missing close double quote (\") character',\n",
       " 'GCP BQ - Cannot read and write in different locations: source: asia-south2, destination: US',\n",
       " 'GCP BQ - Tip: Using Cloud Function to read csv.gz files from github directly to BigQuery in Google Cloud:',\n",
       " 'GCP BQ - When querying two different tables external and materialized you get the same result when count(distinct(*))',\n",
       " 'GCP BQ - How to handle type error from big query and parquet data?',\n",
       " 'GCP BQ - Invalid project ID . Project IDs must contain 6-63 lowercase letters, digits, or dashes. Some project',\n",
       " 'GCP BQ - Does BigQuery support multiple columns partition?',\n",
       " 'GCP BQ - DATE() Error in BigQuery',\n",
       " 'GCP BQ - Native tables vs External tables in BigQuery?',\n",
       " 'GCP BQ ML - Unable to run command (shown in video) to export ML model from BQ to GCS',\n",
       " 'Dim_zones.sql Dataset was not found in location US When Running fact_trips.sql',\n",
       " 'GCP BQ ML - Export ML model to make predictions does not work for MacBook with Apple M1 chip (arm architecture).',\n",
       " 'VMs - What do I do if my VM runs out of space?',\n",
       " \"Homework - What does it mean “Stop with loading the files into a bucket.' Stop with loading the files into a bucket?”\",\n",
       " 'Homework - Reading parquets from nyc.gov directly into pandas returns Out of bounds error',\n",
       " 'Question: for homework 3 , we need all 12 parquet files for green taxi 2022 right ?',\n",
       " 'Homework - Uploading files to GCS via GUI',\n",
       " 'Homework - Qn 5: The partitioned/clustered table isn’t giving me the prediction I expected',\n",
       " 'Homework - Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?',\n",
       " 'Python - invalid start byte Error Message',\n",
       " 'Python - Generators in python',\n",
       " 'Python - Easiest way to read multiple files at the same time?',\n",
       " \"Python - These won't work. You need to make sure you use Int64:\",\n",
       " 'Prefect - Error on Running Prefect Flow to Load data to GCS',\n",
       " 'Prefect - Tip: Downloading csv.gz from a url in a prefect environment (sample snippet).',\n",
       " 'If you are getting not found in location us error.',\n",
       " 'Setup - No development environment',\n",
       " 'Setup - Connecting dbt Cloud with BigQuery Error',\n",
       " 'Dbt build error',\n",
       " 'Setup - Failed to clone repository.',\n",
       " 'dbt job - Triggered by pull requests is disabled when I try to create a new Continuous Integration job in dbt cloud.',\n",
       " 'Setup - Your IDE session was unable to start. Please contact support.',\n",
       " 'DBT - I am having problems with columns datatype while running DBT/BigQuery',\n",
       " 'Ingestion: When attempting to use the provided quick script to load trip data into GCS, you receive error Access Denied from the S3 bucket',\n",
       " 'Ingestion - Error thrown by format_to_parquet_task when converting fhv_tripdata_2020-01.csv using Airflow',\n",
       " 'Hack to load yellow and green trip data for 2019 and 2020',\n",
       " 'Move many files (more than one) from Google cloud storage bucket to Big query',\n",
       " 'GCP VM - All of sudden ssh stopped working for my VM after my last restart',\n",
       " 'GCP VM - If you have lost SSH access to your machine due to lack of space. Permission denied (publickey)',\n",
       " '404 Not found: Dataset eighth-zenith-372015:trip_data_all was not found in location us-west1',\n",
       " 'When executing dbt run after installing dbt-utils latest version i.e., 1.0.0 warning has generated',\n",
       " 'When You are getting error dbt_utils not found',\n",
       " 'Lineage is currently unavailable. Check that your project does not contain compilation errors or contact support if this error persists.',\n",
       " 'Build - Why do my Fact_trips only contain a few days of data?',\n",
       " 'Build - Why do my fact_trips only contain one month of data?',\n",
       " 'BigQuery returns an error when I try to run the dm_monthly_zone_revenue.sql model.',\n",
       " 'Replace: \\n{{ dbt_utils.surrogate_key([ \\n     field_a, \\n     field_b, \\n     field_c,\\n     …,\\n     field_z     \\n]) }}',\n",
       " 'I changed location in dbt, but dbt run still gives me an error',\n",
       " 'I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ.',\n",
       " 'Why do we need the Staging dataset?',\n",
       " 'DBT Docs Served but Not Accessible via Browser',\n",
       " 'BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6',\n",
       " 'Dbt+git - Main branch is “read-only”',\n",
       " \"Dbt+git - It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?\",\n",
       " 'Dbt deploy + Git CI - cannot create CI checks job for deployment to Production. See more discussion in slack chat',\n",
       " 'Dbt deploy + Git CI - Unable to configure Continuous Integration (CI) with Github',\n",
       " \"Compilation Error (Model 'model.my_new_project.stg_green_tripdata' (models/staging/stg_green_tripdata.sql) depends on a source named 'staging.green_trip_external' which was not found)\",\n",
       " \"'NoneType' object is not iterable\",\n",
       " 'dbt macro errors with get_payment_type_description(payment_type)',\n",
       " 'Troubleshooting in dbt:',\n",
       " 'Why changing the target schema to “marts” actually creates a schema named “dbt_marts” instead?',\n",
       " 'How to set subdirectory of the github repository as the dbt project root',\n",
       " \"Compilation Error : Model 'model.XXX' (models/<model_path>/XXX.sql) depends on a source named '<a table name>' which was not found\",\n",
       " \"Compilation Error : Model '<model_name>' (<model_path>) depends on a node named '<seed_name>' which was not found   (Production Environment)\",\n",
       " 'When executing dbt run after using fhv_tripdata as an external table: you get “Access Denied: BigQuery BigQuery: Permission denied”',\n",
       " 'How to automatically infer the column data type (pandas missing value issues)?',\n",
       " 'When loading github repo raise exception that ‘taxi_zone_lookup’ not found',\n",
       " '‘taxi_zone_lookup’ not found',\n",
       " 'Data type errors when ingesting with parquet files',\n",
       " 'Inconsistent number of rows when re-running fact_trips model',\n",
       " 'Data Type Error when running fact table',\n",
       " 'CREATE TABLE has columns with duplicate name locationid.',\n",
       " 'Bad int64 value: 0.0 error',\n",
       " 'Bad int64 value: 2.0/1.0 error',\n",
       " 'DBT - Error on building fact_trips.sql: Parquet column \\'ehail_fee\\' has type DOUBLE which does not match the target cpp_type INT64. File: gs://<gcs bucket>/<table>/green_taxi_2019-01.parquet\")',\n",
       " 'The - vars argument must be a YAML dictionary, but was of type str',\n",
       " 'Not able to change Environment Type as it is greyed out and inaccessible',\n",
       " 'Access Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.',\n",
       " 'Made change to your modelling files and commit the your development branch, but Job still runs on old file?',\n",
       " 'Setup - I’ve set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?',\n",
       " 'Prefect Agent retrieving runs from queue sometimes fails with httpx.LocalProtocolError',\n",
       " 'BigQuery returns an error when i try to run ‘dbt run’:',\n",
       " \"Running dbt run --models stg_green_tripdata --var 'is_test_run: false' is not returning anything:\",\n",
       " \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
       " '\\u200b\\u200bVS Code: NoPermissions (FileSystemError): Error: EACCES: permission denied (linux)',\n",
       " 'Google Cloud BigQuery Location Problems',\n",
       " 'DBT Deploy - This dbt Cloud run was cancelled because a valid dbt project was not found.',\n",
       " 'DBT Deploy + CI - Location Problems on BigQuery',\n",
       " 'DBT Deploy - Error When trying to run the dbt project on Prod',\n",
       " 'DBT - Error: “404 Not found: Dataset <dataset_name>:<dbt_schema_name> was not found in location EU” after building from stg_green_tripdata.sql',\n",
       " 'Homework - Ingesting FHV_20?? data',\n",
       " 'Homework - Ingesting NYC TLC Data',\n",
       " 'How to set environment variable easily for any credentials',\n",
       " \"Invalid date types after Ingesting FHV data through CSV files: Could not parse 'pickup_datetime' as a timestamp\",\n",
       " 'Invalid data types after Ingesting FHV data through parquet files: Could not parse SR_Flag as Float64,Couldn’t parse datetime column as timestamp,couldn’t handle NULL values in PULocationID,DOLocationID',\n",
       " 'Google Looker Studio - you have used up your 30-day trial',\n",
       " 'How does dbt handle dependencies between models?',\n",
       " 'What is the fastest way to upload taxi data to dbt-postgres?',\n",
       " 'When configuring the profiles.yml file for dbt-postgres with jinja templates with environment variables, I\\'m getting \"Credentials in profile \"PROFILE_NAME\", target: \\'dev\\', invalid: \\'5432\\'is not of type \\'integer\\'',\n",
       " 'Setting up Java and Spark (with PySpark) on Linux (Alternative option using SDKMAN)',\n",
       " 'PySpark - Setting Spark up in Google Colab',\n",
       " 'Spark-shell: unable to load native-hadoop library for platform - Windows',\n",
       " 'PySpark - Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.',\n",
       " 'PySpark - TypeError: code() argument 13 must be str, not int  , while executing `import pyspark`  (Windows/ Spark 3.0.3 - Python 3.11)',\n",
       " 'Java+Spark - Easy setup with miniconda env (worked on MacOS)',\n",
       " 'lsRuntimeError: Java gateway process exited before sending its port number',\n",
       " 'Module Not Found Error in Jupyter Notebook .',\n",
       " \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       " \"Py4J Error - ModuleNotFoundError: No module named 'py4j' (Solve with latest version)\",\n",
       " 'Exception: Jupyter command `jupyter-notebook` not found.',\n",
       " 'Error java.io.FileNotFoundException',\n",
       " 'Hadoop - FileNotFoundException: Hadoop bin directory does not exist , when trying to write (Windows)',\n",
       " 'Which type of SQL is used in Spark? Postgres? MySQL? SQL Server?',\n",
       " 'The spark viewer on localhost:4040 was not showing the current run',\n",
       " 'Java - java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call (conda pyspark installation)',\n",
       " 'Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries',\n",
       " 'Spark BigQuery connector Automatic configuration',\n",
       " 'Spark Cloud Storage connector',\n",
       " 'How can I read a small number of rows from the parquet file directly?',\n",
       " 'DataType error when creating Spark DataFrame with a specified schema?',\n",
       " 'Remove white spaces from column names in Pyspark',\n",
       " \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\",\n",
       " \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\",\n",
       " 'Spark Standalone Mode on Windows',\n",
       " 'Export PYTHONPATH command in linux is temporary',\n",
       " 'Compressed file ended before the end-of-stream marker was reached',\n",
       " 'Compression Error: zcat output is gibberish, seems like still compressed',\n",
       " 'PicklingError: Could not serialise object: IndexError: tuple index out of range.',\n",
       " 'Connecting from local Spark to GCS - Spark does not find my google credentials as shown in the video?',\n",
       " 'Spark docker-compose setup',\n",
       " 'How do you read data stored in gcs on pandas with your local computer?',\n",
       " 'TypeError when using spark.createDataFrame function on a pandas df',\n",
       " 'MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory',\n",
       " 'How to spark standalone cluster is run on windows OS',\n",
       " 'Env variables set in ~/.bashrc are not loaded to Jupyter in VS Code',\n",
       " 'How to port forward outside VS Code',\n",
       " '“wc -l” is giving a different result then shown in the video',\n",
       " '`spark-submit` errors',\n",
       " 'Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z',\n",
       " 'Java.io.IOException. Cannot run program “C:\\\\hadoop\\\\bin\\\\winutils.exe”. CreateProcess error=216, This version of 1% is not compatible with the version of Windows you are using.',\n",
       " 'Dataproc - ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [project] is not currently set. It can be set on a per-command basis by re-running your command with the [--project] flag.',\n",
       " 'Run Local Cluster Spark in Windows 10 with CMD',\n",
       " \"lServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist).\",\n",
       " 'py4j.protocol.Py4JJavaError  GCP',\n",
       " 'Repartition the Dataframe to 6 partitions using df.repartition(6) - got 8 partitions instead',\n",
       " 'Jupyter Notebook or SparkUI not loading properly at localhost after port forwarding from VS code?',\n",
       " 'Installing Java 11 on codespaces',\n",
       " \"Error: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 470.0.\",\n",
       " 'Homework - how to convert the time difference of two timestamps to hours',\n",
       " 'PicklingError: Could not serialize object: IndexError: tuple index out of range',\n",
       " 'Py4JJavaError: An error occurred while calling o180.showString. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.',\n",
       " 'RuntimeError: Python in worker has different version 3.11 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.',\n",
       " 'Dataproc Qn: Is it essential to have a VM on GCP for running Dataproc and submitting jobs ?',\n",
       " 'In module 5.3.1, trying to run spark.createDataFrame(df_pandas).show() returns error',\n",
       " 'Setting JAVA_HOME with Homebrew on Apple Silicon',\n",
       " 'Could not start docker image “control-center” from the docker-compose.yaml file.',\n",
       " 'Module “kafka” not found when trying to run producer.py',\n",
       " 'Error importing cimpl dll when running avro examples',\n",
       " \"ModuleNotFoundError: No module named 'avro'\",\n",
       " 'Error while running python3 stream.py worker',\n",
       " 'Negsignal:SIGKILL while converting dta files to parquet format',\n",
       " 'data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing',\n",
       " 'Kafka- python videos have low audio and hard to follow up',\n",
       " 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable',\n",
       " 'Kafka homwork Q3, there are options that support scaling concept more than the others:',\n",
       " \"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\",\n",
       " 'Python Kafka: ./build.sh: Permission denied Error',\n",
       " 'Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py',\n",
       " 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.',\n",
       " 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails',\n",
       " 'Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.',\n",
       " 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build',\n",
       " 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py',\n",
       " 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal',\n",
       " 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent',\n",
       " 'Java Kafka: Tests are not picked up in VSCode',\n",
       " 'Confluent Kafka: Where can I find schema registry URL?',\n",
       " 'How do I check compatibility of local and container Spark versions?',\n",
       " 'How to fix the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\"?',\n",
       " 'How is my capstone project going to be evaluated?',\n",
       " 'Project 1 & Project 2',\n",
       " 'Does anyone know nice and relatively large datasets?',\n",
       " 'How to run python as start up script?',\n",
       " 'Spark Streaming - How do I read from multiple topics in the same Spark Session',\n",
       " 'Data Transformation from Databricks to Azure SQL DB',\n",
       " 'Orchestrating dbt with Airflow',\n",
       " 'Orchestrating DataProc with Airflow',\n",
       " 'Orchestrating dbt cloud with Mage',\n",
       " 'Project evaluation - Reproducibility',\n",
       " 'Key Vault in Azure cloud stack',\n",
       " \"Spark docker - `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       " 'psycopg2 complains of incompatible environment e.g x86 instead of amd',\n",
       " 'Setting up dbt locally with Docker and Postgres',\n",
       " 'How to connect Pyspark with BigQuery?',\n",
       " 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key',\n",
       " 'Edit Course Profile.',\n",
       " 'How do I install the necessary dependencies to run the code?',\n",
       " 'Other packages needed but not listed',\n",
       " 'How can I use DuckDB In-Memory database with dlt ?',\n",
       " 'Homework - dlt Exercise 3 - Merge a generator concerns',\n",
       " 'command.sh Error - source: no such file or directory: command.sh',\n",
       " 'psql - command not found: psql (alternative install)',\n",
       " 'Setup - source command.sh - error: “docker-compose” not found',\n",
       " 'Setup - start-cluster error: Invalid top-level property x-image',\n",
       " 'stream-kafka Qn: Is it expected that the records are being ingested 10 at a time?',\n",
       " 'Setup - Qn: Is kafka install required for the RisingWave workshop? [source]',\n",
       " 'Setup - Qn: How much free disk space should we have? [source]',\n",
       " 'Psycopg2 - issues when running stream-kafka script',\n",
       " 'Psycopg2 - `Could not build wheels for psycopg2, which is required to install pyproject.toml-based projects`',\n",
       " 'Psycopg2 InternalError: Failed to run the query - when running the seed-kafka command after initial setup.',\n",
       " 'Running stream-kafka script gets stuck on a loop with Connection Refused',\n",
       " 'For the homework questions is there a specific number of records that have to be processed to obtain the final answer?',\n",
       " 'Homework - Materialized view does not guarantee order by warning',\n",
       " 'How to install postgress on Linux like OS',\n",
       " 'Unable to Open Dashboard as xdg-open doesn’t open any browser',\n",
       " 'Resolving Python Interpreter Path Inconsistencies in Unix-like Environments',\n",
       " 'How does windowing work in Sql?',\n",
       " 'Encountering the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'\" when running \"from kafka import KafkaProducer\" in Jupyter Notebook for Module 6 Homework?',\n",
       " 'Basic Commands',\n",
       " 'How do I sign up?',\n",
       " 'Is it going to be live? When?',\n",
       " 'What if I miss a session?',\n",
       " 'How much theory will you cover?',\n",
       " \"I don't know math. Can I take the course?\",\n",
       " \"I filled the form, but haven't received a confirmation email. Is it normal?\",\n",
       " 'How long is the course?',\n",
       " 'How much time do I need for this course?',\n",
       " 'Will I get a certificate?',\n",
       " 'Will I get a certificate if I missed the midterm project?',\n",
       " 'How much Python should I know?',\n",
       " \"Any particular hardware requirements for the course or everything is mostly cloud? TIA! Couldn't really find this in the FAQ.\",\n",
       " 'How to setup TensorFlow with GPU support on Ubuntu?',\n",
       " 'I’m new to Slack and can’t find the course channel. Where is it?',\n",
       " 'The course has already started. Can I still join it?',\n",
       " 'When does the next iteration start?',\n",
       " 'Can I submit the homework after the due date?',\n",
       " 'I just joined. What should I do next? How can I access course materials?',\n",
       " 'What are the deadlines in this course?',\n",
       " 'What’s the difference between the previous iteration of the course (2022) and this one (2023)?',\n",
       " 'The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021?',\n",
       " 'Submitting learning in public links',\n",
       " 'Adding community notes',\n",
       " 'Computing the hash for the leaderboard and project review',\n",
       " 'wget is not recognized as an internal or external command',\n",
       " 'Retrieving csv inside notebook',\n",
       " 'Windows WSL and VS Code\\nIf you have a Windows 11 device and would like to use the built in WSL to access linux you can use the Microsoft Learn link Set up a WSL development environment | Microsoft Learn. To connect this to VS Code download the Microsoft verified VS Code extension ‘WSL’ this will allow you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine.',\n",
       " 'Uploading the homework to Github',\n",
       " 'Singular Matrix Error',\n",
       " 'Conda is not an internal command',\n",
       " 'Read-in the File in Windows OS',\n",
       " \"'403 Forbidden' error message when you try to push to a GitHub repository\",\n",
       " \"Fatal: Authentication failed for 'https://github.com/username\",\n",
       " \"wget: unable to resolve host address 'raw.githubusercontent.com'\",\n",
       " 'Setting up an environment using VS Code',\n",
       " 'Conda Environment Setup',\n",
       " 'Floating Point Precision',\n",
       " 'What does pandas.DataFrame.info() do?',\n",
       " \"NameError: name 'np' is not defined\",\n",
       " 'How to select column by dtype',\n",
       " 'How to identify the shape of dataset in Pandas',\n",
       " 'How to avoid Value errors with array shapes in homework?',\n",
       " 'Question 5: How and why do we replace the NaN values with average of the column?',\n",
       " 'Question 7: Mathematical formula for linear regression',\n",
       " 'Question 7: FINAL MULTIPLICATION not having 5 column',\n",
       " 'Question 7: Multiplication operators.',\n",
       " 'Error launching Jupyter notebook',\n",
       " 'wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1',\n",
       " 'In case you are using mac os and having trouble with WGET',\n",
       " 'How to output only a certain number of decimal places',\n",
       " 'How do I get started with Week 2?',\n",
       " 'Checking long tail of data',\n",
       " 'LinAlgError: Singular matrix',\n",
       " 'California housing dataset',\n",
       " 'Getting NaNs after applying .mean()',\n",
       " 'Target variable transformation',\n",
       " 'Reading the dataset directly from github',\n",
       " 'Loading the dataset directly through Kaggle Notebooks',\n",
       " 'Filter a dataset by using its values',\n",
       " 'Alternative way to load the data using requests',\n",
       " 'Null column is appearing even if I applied .fillna()',\n",
       " 'Can I use Scikit-Learn’s train_test_split for this week?',\n",
       " 'Can I use LinearRegression from Scikit-Learn for this week?',\n",
       " 'Corresponding Scikit-Learn functions for Linear Regression (with and without Regularization)',\n",
       " 'Question 4: what is `r`, is it the same as `alpha` in sklearn.Ridge()?',\n",
       " 'Why linear regression doesn’t provide a “perfect” fit?',\n",
       " 'Random seed 42',\n",
       " 'Shuffling the initial dataset using pandas built-in function',\n",
       " \"The answer I get for one of the homework questions doesn't match any of the options. What should I do?\",\n",
       " 'Meaning of mean in homework 2, question 3',\n",
       " 'When should we transform the target variable to logarithm distribution?',\n",
       " 'ValueError: shapes not aligned',\n",
       " 'How to copy a dataframe without changing the original dataframe?',\n",
       " 'What does ‘long tail’ mean?',\n",
       " 'What is standard deviation?',\n",
       " 'Do we need to apply regularization techniques always? Or only in certain scenarios?',\n",
       " 'Shortcut: define functions for faster execution',\n",
       " 'How to use pandas to find standard deviation',\n",
       " 'Standard Deviation Differences in Numpy and Pandas',\n",
       " 'Standard deviation using Pandas built in Function',\n",
       " 'How to combine train and validation datasets',\n",
       " 'Understanding RMSE and how to calculate RMSE score',\n",
       " 'What syntax use in Pandas for multiple conditions using logical AND and OR',\n",
       " 'Deep dive into normal equation for regression',\n",
       " 'Useful Resource for Missing Data Treatment\\nhttps://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook',\n",
       " 'Caution for applying log transformation in Week-2 2023 cohort homework',\n",
       " 'What sklearn version is Alexey using in the youtube videos?',\n",
       " 'How do I get started with Week 3?',\n",
       " \"Could not convert string to float:’Nissan’rt string to float: 'Nissan'\",\n",
       " 'Why did we change the targets to binary format when calculating mutual information score in the homework?',\n",
       " 'What data should we use for correlation matrix',\n",
       " 'Coloring the background of the pandas.DataFrame.corr correlation matrix directly',\n",
       " 'Identifying highly correlated feature pairs easily through unstack',\n",
       " 'What data should be used for EDA?',\n",
       " 'Fitting DictVectorizer on validation',\n",
       " 'Feature elimination',\n",
       " 'FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2',\n",
       " 'Logistic regression crashing Jupyter kernel',\n",
       " 'Understanding Ridge',\n",
       " 'pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings:',\n",
       " 'Convergence Problems in W3Q6',\n",
       " 'Dealing with Convergence in Week 3 q6',\n",
       " 'Sparse matrix compared dense matrix',\n",
       " 'How  to Disable/avoid Warnings in Jupyter Notebooks',\n",
       " 'How to select the alpha parameter in Q6',\n",
       " 'Second variable that we need to use to calculate the mutual information score',\n",
       " 'Features for homework Q5',\n",
       " 'What is the difference between OneHotEncoder and DictVectorizer?',\n",
       " 'What is the difference between pandas get_dummies and sklearn OnehotEncoder?',\n",
       " 'Use of random seed in HW3',\n",
       " 'Correlation before or after splitting the data',\n",
       " 'Features in Ridge Regression Model',\n",
       " 'Handling Column Information for Homework 3 Question 6',\n",
       " 'Transforming Non-Numerical Columns into Numerical Columns',\n",
       " 'What is the better option FeatureHasher or DictVectorizer',\n",
       " \"Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way?\",\n",
       " 'HW3Q4 I am getting 1.0 as accuracy. Should I use the closest option?',\n",
       " 'How to calculate Root Mean Squared Error?',\n",
       " \"AttributeError: 'DictVectorizer' object has no attribute 'get_feature_names'\",\n",
       " 'Root Mean Squared Error',\n",
       " 'Encoding Techniques',\n",
       " 'Error in use of accuracy_score from sklearn in jupyter (sometimes)',\n",
       " 'How do I get started with Week 4?',\n",
       " 'Using a variable to score',\n",
       " 'Why do we sometimes use random_state and not at other times?',\n",
       " 'How to get all classification metrics?',\n",
       " 'Multiple thresholds for Q4',\n",
       " 'ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0',\n",
       " 'Method to get beautiful classification report',\n",
       " 'I’m not getting the exact result in homework',\n",
       " 'Use AUC to evaluate feature importance of numerical variables',\n",
       " 'Help with understanding: “For each numerical value, use it as score and compute AUC”',\n",
       " 'What dataset should I use to compute the metrics in Question 3',\n",
       " 'What does KFold do?',\n",
       " \"ValueError: multi_class must be in ('ovo', 'ovr')\",\n",
       " 'Monitoring Wait times and progress of the code execution can be done with:',\n",
       " 'What is the use of inverting or negating the variables less than the threshold?',\n",
       " 'Difference between predict(X) and predict_proba(X)[:, 1]',\n",
       " 'Why are FPR and TPR equal to 0.0, when threshold = 1.0?',\n",
       " 'How can I annotate a graph?',\n",
       " 'I didn’t fully understand the ROC curve. Can I move on?',\n",
       " 'Why do I have different values of accuracy than the options in the homework?',\n",
       " 'How to find the intercept between precision and recall curves by using numpy?',\n",
       " 'Compute Recall, Precision, and F1 Score using scikit-learn library',\n",
       " 'Why do we use cross validation?',\n",
       " 'Evaluate the Model using scikit learn metrics',\n",
       " 'Are there other ways to compute Precision, Recall and F1 score?',\n",
       " 'When do I use ROC vs Precision-Recall curves?',\n",
       " 'How to evaluate feature importance for numerical variables with AUC?',\n",
       " 'Dependence of the F-score on class imbalance',\n",
       " 'Quick way to plot Precision-Recall Curve',\n",
       " 'What is Stratified k-fold?',\n",
       " 'How do I get started with Week 5?',\n",
       " 'Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc.',\n",
       " 'How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience',\n",
       " 'Basic Ubuntu Commands:',\n",
       " 'Installing and updating to the python version 3.10 and higher',\n",
       " 'How to install WSL on Windows 10 and 11 ?',\n",
       " 'Error building Docker images on Mac with M1 silicon',\n",
       " 'Method to find the version of any install python libraries in jupyter notebook',\n",
       " 'Cannot connect to the docker daemon. Is the Docker daemon running?',\n",
       " \"The command '/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache' returned a non-zero code: 1\",\n",
       " 'Running “pipenv install sklearn==1.0.2” gives errors. What should I do?',\n",
       " 'Why do we need the --rm flag',\n",
       " 'Failed to read Dockerfile',\n",
       " 'Install docker on MacOS',\n",
       " 'I cannot pull the image with docker pull command',\n",
       " 'Dumping/Retrieving only the size of for a specific Docker image',\n",
       " 'Where does pipenv create environments and how does it name them?',\n",
       " 'How do I debug a docker container?',\n",
       " 'The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash)',\n",
       " 'Error: failed to compute cache key: \"/model2.bin\" not found: not found',\n",
       " 'Failed to write the dependencies to pipfile and piplock file',\n",
       " 'f-strings',\n",
       " \"'pipenv' is not recognized as an internal or external command, operable program or batch file.\",\n",
       " 'AttributeError: module ‘collections’ has no attribute ‘MutableMapping’',\n",
       " \"Q: ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts')\",\n",
       " \"ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\",\n",
       " 'docker  build ERROR [x/y] COPY …',\n",
       " 'Fix error during installation of Pipfile inside Docker container',\n",
       " 'How to fix error after running the Docker run command',\n",
       " 'Bind for 0.0.0.0:9696 failed: port is already allocated',\n",
       " 'Bind for 127.0.0.1:5000 showing error',\n",
       " 'Installing md5sum on Macos',\n",
       " 'How to run a script while a web-server is working?',\n",
       " 'Version-conflict in pipenv',\n",
       " 'Python_version and Python_full_version error after running pipenv install:',\n",
       " 'Your Pipfile.lock (221d14) is out of date (during Docker build)',\n",
       " 'You are using windows. Conda environment. You then use waitress instead of gunicorn. After a few runs, suddenly mlflow server fails to run.',\n",
       " 'Completed creating the environment locally but could not find the environment on AWS.',\n",
       " 'Installing waitress on Windows via GitBash: “waitress-serve” command not found',\n",
       " 'Warning: the environment variable LANG is not set!',\n",
       " 'Module5 HW Question 6',\n",
       " 'Terminal Used in Week 5 videos:',\n",
       " 'waitress-serve shows Malformed application',\n",
       " 'Testing HTTP POST requests from command line using curl',\n",
       " 'NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.',\n",
       " \"Requests Error: No connection adapters were found for 'localhost:9696/predict'.\",\n",
       " 'Getting the same result',\n",
       " 'Trying to run a docker image I built but it says it’s unable to start the container process',\n",
       " 'How do I copy files from my local machine to docker container?',\n",
       " 'How do I copy files from a different folder into docker container’s working directory?',\n",
       " 'I can’t create the environment on AWS Elastic Beanstalk with the command proposed during the video',\n",
       " 'Dockerfile missing when creating the AWS ElasticBean environment',\n",
       " 'How to get started with Week 6?',\n",
       " 'How to get the training and validation metrics from XGBoost?',\n",
       " 'How to solve regression problems with random forest in scikit-learn?',\n",
       " 'ValueError: feature_names must be string, and may not contain [, ] or <',\n",
       " \"`TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> ` when training xgboost model.\",\n",
       " 'Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=)',\n",
       " 'How to Install Xgboost',\n",
       " 'What is eta in XGBoost',\n",
       " 'What is the difference between bagging and boosting?',\n",
       " 'Capture stdout for each iterations of a loop separately',\n",
       " 'ValueError: continuous format is not supported',\n",
       " 'Question 3 of homework 6 if i see that rmse goes up at a certain number of n_estimators but then goes back down lower than it was before, should the answer be the number of n_estimators after which rmse initially went up, or the number after which it was its overall lowest value?',\n",
       " 'One of the method to visualize the decision trees',\n",
       " \"ValueError: Unknown label type: 'continuous'\",\n",
       " 'Different values of auc, each time code is re-run',\n",
       " 'Does it matter if we let the Python file create the server or if we run gunicorn directly?',\n",
       " 'No module named ‘ping’?',\n",
       " 'DictVectorizer feature names',\n",
       " 'Does it matter if we let the Python file create the server or if we run gunicorn directly?',\n",
       " 'ValueError: feature_names must be string, and may not contain [, ] or <',\n",
       " 'Visualize Feature Importance by using horizontal bar chart',\n",
       " 'RMSE using metrics.root_meas_square()',\n",
       " 'Features Importance graph',\n",
       " 'xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks.',\n",
       " 'Information Gain',\n",
       " 'Data Leakage',\n",
       " 'Serialized Model Xgboost error',\n",
       " 'How to get started with Week 8?',\n",
       " 'How to use Kaggle for Deep Learning?',\n",
       " 'How to use Google Colab for Deep Learning?',\n",
       " 'How do I push from Saturn Cloud to Github?',\n",
       " 'Where is the Python TensorFlow template on Saturn Cloud?',\n",
       " 'Getting error module scipy not found during model training in Saturn Cloud tensorflow image',\n",
       " 'How to upload kaggle data to Saturn Cloud?',\n",
       " 'How to install CUDA & cuDNN on Ubuntu 22.04',\n",
       " 'Error: (ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.) when loading model.',\n",
       " 'Getting error when connect git on Saturn Cloud: permission denied',\n",
       " 'Host key verification failed.',\n",
       " 'The same accuracy on epochs',\n",
       " 'Model breaking after augmentation – high loss + bad accuracy',\n",
       " 'Missing channel value error while reloading model:',\n",
       " 'How to unzip a folder with an image dataset and suppress output?',\n",
       " 'How keras flow_from_directory know the names of classes in images?',\n",
       " 'Error with scipy missing module in SaturnCloud',\n",
       " 'How are numeric class labels determined in flow_from_directroy using binary class mode and what is meant by the single probability predicted by a binary Keras model:',\n",
       " 'Does the actual values matter after predicting with a neural network or it should be treated as like hood of falling in a class?',\n",
       " 'What if your accuracy and std training loss don’t match HW?',\n",
       " 'Using multi-threading for data generation in “model.fit()”',\n",
       " 'Reproducibility with TensorFlow using a seed point',\n",
       " 'Can we use pytorch for this lesson/homework ?',\n",
       " 'Keras model training fails with “Failed to find data adapter”',\n",
       " 'Running ‘nvidia-smi’ in a loop without using ‘watch’',\n",
       " 'Checking GPU and CPU utilization using ‘nvitop’',\n",
       " 'Q: Where does the number of Conv2d layer’s params come from? Where does the number of “features” we get after the Flatten layer come from?',\n",
       " 'Sequential vs. Functional Model Modes in Keras (TF2)',\n",
       " 'Out of memory errors when running tensorflow',\n",
       " 'Model training very slow in google colab with T4 GPU',\n",
       " 'Using image_dataset_from_directory instead of ImageDataGeneratorn for loading images',\n",
       " 'How to get started with Week 9?',\n",
       " 'Where is the model for week 9?',\n",
       " 'Executing the command echo ${REMOTE_URI} returns nothing.',\n",
       " 'Getting a syntax error while trying to get the password from aws-cli',\n",
       " 'Pass many parameters in the model at once',\n",
       " 'Getting  ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8',\n",
       " \"Problem: 'ls' is not recognized as an internal or external command, operable program or batch file.\",\n",
       " 'ImportError: generic_type: type \"InterpreterWrapper\" is already registered!',\n",
       " 'Windows version might not be up-to-date',\n",
       " 'WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available',\n",
       " 'How to do AWS configure after installing awscli',\n",
       " 'Object of type float32 is not JSON serializable',\n",
       " 'Error with the line “interpreter.set_tensor(input_index, X”)',\n",
       " 'How to easily get file size in powershell terminal ?',\n",
       " 'How do Lambda container images work?',\n",
       " 'How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService?',\n",
       " 'Error building docker image on M1 Mac',\n",
       " 'Error invoking API Gateway deploy API locally',\n",
       " 'Error: Could not find a version that satisfies the requirement tflite_runtime (from versions:none)',\n",
       " 'Docker run error',\n",
       " 'Save Docker Image to local machine and view contents',\n",
       " 'Jupyter notebook not seeing package',\n",
       " 'Running out of space for AWS instance.',\n",
       " 'Using Tensorflow 2.15 for AWS deployment',\n",
       " 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”',\n",
       " 'What IAM permission policy is needed to complete Week 9: Serverless?',\n",
       " 'Docker Temporary failure in name resolution',\n",
       " 'Keras model *.h5 doesn’t load. Error: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`',\n",
       " 'How to test AWS Lambda + Docker locally?',\n",
       " '\"Unable to import module \\'lambda_function\\': No module named \\'tensorflow\\'\" when run python test.py',\n",
       " 'Install Docker (udocker) in Google Colab',\n",
       " 'How to get started with Week 10?',\n",
       " 'How to install Tensorflow in Ubuntu WSL2',\n",
       " 'Getting: Allocator ran out of memory errors?',\n",
       " 'Problem with recent version of protobuf',\n",
       " 'WSL Cannot Connect To Docker Daemon',\n",
       " 'HPA instance doesn’t run properly',\n",
       " 'HPA instance doesn’t run properly (easier solution)',\n",
       " 'Could not install packages due to an OSError: [WinError 5] Access is denied',\n",
       " 'TypeError: Descriptors cannot not be created directly.',\n",
       " 'How to install easily kubectl on windows ?',\n",
       " 'Install kind through choco library',\n",
       " 'Install Kind via Go package',\n",
       " 'The connection to the server localhost:8080 was refused - did you specify the right host or port?',\n",
       " 'Running out of storage after building many docker images',\n",
       " 'In HW10 Q6 what does it mean “correct value for CPU and memory”? Aren’t they arbitrary?',\n",
       " 'Why cpu vals for Kubernetes deployment.yaml look like “100m” and “500m”? What does \"m\" mean?',\n",
       " 'Kind cannot load docker image',\n",
       " \"'kind' is not recognized as an internal or external command, operable program or batch file. (In Windows)\",\n",
       " 'Running kind on Linux with Rootless Docker or Rootless Podman',\n",
       " 'Kubernetes-dashboard',\n",
       " 'Correct AWS CLI version for eksctl',\n",
       " \"TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask\",\n",
       " 'Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”',\n",
       " 'Error downloading  tensorflow/serving:2.7.0 on Apple M1 Mac',\n",
       " 'Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well)',\n",
       " 'HPA doesn’t show CPU metrics',\n",
       " 'Errors with istio during installation',\n",
       " 'Problem title',\n",
       " 'What are the project deadlines?',\n",
       " 'Are projects solo or collaborative/group work?',\n",
       " 'What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz?',\n",
       " 'Crucial Links',\n",
       " 'How to conduct peer reviews for projects?',\n",
       " 'Computing the hash for project review',\n",
       " 'Learning in public links for the projects',\n",
       " \"My dataset is too large and I can't loaded in GitHub , do anyone knows about a solution?\",\n",
       " 'What If I submitted only two projects and failed to submit the third?',\n",
       " \"I did the first two projects and skipped the last one so I wouldn't have two peer review in second capstone right?\",\n",
       " 'How many models should I train?',\n",
       " 'How does the project evaluation work for you as a peer reviewer?',\n",
       " 'Do you pass a project based on the average of everyone else’s scores or based on the total score you earn?',\n",
       " 'Why do I need to provide a train.py file when I already have the notebook.ipynb file?',\n",
       " 'Loading the Image with PILLOW library and converting to numpy array',\n",
       " 'Is a train.py file necessary when you have a train.ipynb file in your midterm project directory?',\n",
       " 'Is there a way to serve up a form for users to enter data for the model to crunch on?',\n",
       " 'How to get feature importance for XGboost model',\n",
       " '[Errno 12] Cannot allocate memory in AWS Elastic Container Service',\n",
       " 'Pickle error: can’t get attribute XXX on module __main__',\n",
       " 'How to handle outliers in a dataset?',\n",
       " 'Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named \\'sklearn\\'',\n",
       " 'BentoML not working with –production flag at any stage: e.g. with bentoml serve and while running the bentoml container',\n",
       " 'Reproducibility',\n",
       " 'Model too big',\n",
       " 'Permissions to push docker to Google Container Registry',\n",
       " 'Tflite_runtime unable to install',\n",
       " 'Error when running ImageDataGenerator.flow_from_dataframe',\n",
       " 'How to pass BentoML content / docker container to Amazon Lambda',\n",
       " 'Error UnidentifiedImageError: cannot identify image file',\n",
       " '[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies',\n",
       " 'Get_feature_names() not found',\n",
       " 'Error decoding JSON response: Expecting value: line 1 column 1 (char 0)',\n",
       " 'Free cloud alternatives',\n",
       " 'Getting day of the year from day and month column',\n",
       " 'Chart for classes and predictions',\n",
       " 'Convert dictionary values to Dataframe table',\n",
       " 'Kitchenware Classification Competition Dataset Generator',\n",
       " 'CUDA toolkit and cuDNN Install for Tensorflow',\n",
       " 'About getting the wrong result when multiplying matrices',\n",
       " 'None of the videos have how to install the environment in Mac, does someone have instructions for Mac with M1 chip?',\n",
       " 'I may end up submitting the assignment late. Would it be evaluated?',\n",
       " 'Does the github repository need to be public?',\n",
       " 'How to use wget with Google Colab?',\n",
       " 'Features in scikit-learn?',\n",
       " 'When I plotted using Matplot lib to check if median has a tail, I got the error below how can one bypass?',\n",
       " 'Reproducibility in different OS',\n",
       " 'Deploying to Digital Ocean',\n",
       " 'Is it best to train your model only on the most important features?',\n",
       " 'How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows?',\n",
       " 'Can I do the course in other languages, like R or Scala?',\n",
       " 'Is use of libraries like fast.ai or huggingface allowed in the capstone and competition, or are they considered to be \"too much help\"?',\n",
       " 'Flask image was built and tested successfully, but tensorflow serving image was built and unable to test successfully. What could be the problem?',\n",
       " 'Any advice for adding the Machine Learning Zoomcamp experience to your LinkedIn profile?',\n",
       " 'Format for questions: [Problem title]',\n",
       " 'What is the expected duration of this course or that for each module?',\n",
       " 'What’s the difference between the 2023 and 2022 course?',\n",
       " 'Will there be a 2024 Cohort? When will the 2024 cohort start?',\n",
       " 'What if my answer is not exactly the same as the choices presented?',\n",
       " 'Are we free to choose our own topics for the final project?',\n",
       " 'Can I still graduate when I didn’t complete homework for week x?',\n",
       " 'For the final project, is it required to be put on the cloud?',\n",
       " 'Port-forwarding without Visual Studio',\n",
       " 'Opening Jupyter in VSCode',\n",
       " 'Configuring Github to work from the remote VM',\n",
       " 'Opening Jupyter in AWS',\n",
       " 'WSL instructions',\n",
       " '.gitignore how-to',\n",
       " 'AWS suggestions',\n",
       " 'IBM Cloud an alternative for AWS',\n",
       " 'AWS costs',\n",
       " 'Is the AWS free tier enough for doing this course?',\n",
       " 'AWS EC2: this site can’t be reached',\n",
       " 'Unprotected private key file!',\n",
       " 'AWS EC2 instance constantly drops SSH connection',\n",
       " 'AWS EC2 IP Update',\n",
       " 'VS Code crashes when connecting to Jupyter',\n",
       " 'X has 526 features, but expecting 525 features',\n",
       " 'Missing dependencies',\n",
       " 'No RMSE value in the options',\n",
       " 'How to replace distplot with histplot',\n",
       " \"KeyError: 'PULocationID'  or  'DOLocationID'\",\n",
       " 'Reading large parquet files',\n",
       " 'Distplot takes too long',\n",
       " 'RMSE on test set too high',\n",
       " 'Q: Using of OneHotEncoder instead of DictVectorizer',\n",
       " 'Q: Why did we not use OneHotEncoder(sklearn) instead of DictVectorizer ?',\n",
       " 'Clipping outliers',\n",
       " 'Replacing NaNs for pickup location and drop off location with -1 for One-Hot Encoding',\n",
       " 'Slightly different RSME',\n",
       " 'Extremely low RSME',\n",
       " 'Enabling Auto-completion in jupyter notebook',\n",
       " 'Downloading the data from the NY Taxis datasets gives error : 403 Forbidden',\n",
       " 'Using PyCharm & Conda env in remote development',\n",
       " 'Running out of memory',\n",
       " 'Activating Anaconda env in .bashrc',\n",
       " 'The feature size is different for training set and validation set',\n",
       " 'Permission denied (publickey) Error (when you remove your public key on the AWS machine)',\n",
       " 'Overfitting: Absurdly high RMSE on the validation dataset',\n",
       " 'Can’t import sklearn',\n",
       " 'Access Denied at Localhost:5000 - Authorization Issue',\n",
       " \"Connection in use: ('127.0.0.1', 5000)\",\n",
       " 'Could not convert string to float - ValueError',\n",
       " 'Experiment not visible in MLflow UI',\n",
       " 'Hash Mismatch Error with Package Installation',\n",
       " 'How to Delete an Experiment Permanently from MLFlow UI',\n",
       " 'How to Update Git Public Repo Without Overwriting Changes',\n",
       " 'Image size of 460x93139 pixels is too large. It must be less than 2^16 in each direction.',\n",
       " \"MlflowClient object has no attribute 'list_experiments'\",\n",
       " 'MLflow Autolog not working',\n",
       " 'MLflow URL (http://127.0.0.1:5000), doesn’t open.',\n",
       " 'MLflow.xgboost Autolog Model Signature Failure',\n",
       " 'MlflowException: Unable to Set a Deleted Experiment',\n",
       " 'No Space Left on Device - OSError[Errno 28]',\n",
       " 'Parameters Mismatch in Homework Q3',\n",
       " 'Protobuf error when installing MLflow',\n",
       " 'Setting up Artifacts folders',\n",
       " 'Setting up MLflow experiment tracker on GCP',\n",
       " 'Setuptools Replacing Distutils - MLflow Autolog Warning',\n",
       " 'Sorting runs in MLflow UI',\n",
       " \"TypeError: send_file() unexpected keyword 'max_age' during MLflow UI Launch\",\n",
       " 'mlflow ui on Windows FileNotFoundError: [WinError 2] The system cannot find the file specified',\n",
       " 'Unsupported Operand Type Error in hpo.py',\n",
       " 'Unsupported Scikit-Learn version',\n",
       " 'Mlflow CLI does not return experiments',\n",
       " 'Viewing MLflow Experiments using MLflow CLI',\n",
       " 'Viewing SQLlite Data Raw & Deleting Experiments Manually',\n",
       " 'What does launching the tracking server locally mean?',\n",
       " 'Parameter adding in case of max_depth not recognized',\n",
       " 'Max_depth is not recognize even when I add the mlflow.log_params',\n",
       " \"AttributeError: 'tuple' object has no attribute 'tb_frame'\",\n",
       " 'WandB API error',\n",
       " 'WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.',\n",
       " 'wget not working',\n",
       " 'Open/run github notebook(.ipynb) directly in Google Colab',\n",
       " 'Why do we use Jan/Feb/March for Train/Test/Validation Purposes?',\n",
       " 'Problem title',\n",
       " 'Where is the FAQ for Prefect questions?',\n",
       " 'aws.exe: error: argument operation: Invalid choice — Docker can not login to ECR.',\n",
       " 'Multiline commands in Windows Powershell',\n",
       " \"Pipenv installation not working (AttributeError: module 'collections' has no attribute 'MutableMapping')\",\n",
       " \"module is not available (Can't connect to HTTPS URL)\",\n",
       " \"No module named 'pip._vendor.six'\",\n",
       " 'Pipenv with Jupyter',\n",
       " 'Pipenv with Jupyter no output',\n",
       " '‘Invalid base64’ error after running `aws kinesis put-record`',\n",
       " 'Error index 311297 is out of bounds for axis 0 with size 131483 when loading parquet file.',\n",
       " 'Pipfile.lock was not created along with Pipfile',\n",
       " 'Permission Denied using Pipenv',\n",
       " \"Error while parsing arguments via CLI  [ValueError: Unknown format code 'd' for object of type 'str']\",\n",
       " 'Dockerizing tips',\n",
       " 'Running multiple services in a Docker container',\n",
       " 'Cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError)',\n",
       " 'Connecting s3 bucket to MLFLOW',\n",
       " 'Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\"',\n",
       " 'Dockerizing lightgbm',\n",
       " 'Error raised when executing mlflow’s pyfunc.load_model in lambda function.',\n",
       " '4.3 FYI Notebook is end state of Video -',\n",
       " 'Passing envs to my docker image',\n",
       " 'How to see the model in the docker container in app/?',\n",
       " \"WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\",\n",
       " 'HTTPError: HTTP Error 403: Forbidden when call apply_model() in score.ipynb',\n",
       " \"ModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'\",\n",
       " 'Login window in Grafana',\n",
       " 'Error in starting monitoring services in Linux',\n",
       " 'KeyError ‘content-length’ when running prepare.py',\n",
       " 'Evidently service exit with code 2',\n",
       " 'ValueError: Incorrect item instead of a metric or metric preset was passed to Report',\n",
       " 'For the report RegressionQualityMetric()',\n",
       " 'Found array with 0 sample(s)',\n",
       " 'Adding additional metric',\n",
       " 'Standard login in Grafana does not work',\n",
       " 'The chart in Grafana doesn’t get updates',\n",
       " 'Prefect server was not running locally',\n",
       " 'no disk space left error when doing docker compose up',\n",
       " 'Failed to listen on :::8080 (reason: php_network_getaddresses: getaddrinfo failed: Address family for hostname not supported)',\n",
       " 'Generate Evidently Chart in Grafana',\n",
       " 'Get an error ‘Unable to locate credentials’ after running localstack with kinesis',\n",
       " 'Get an error ‘ unspecified location constraint is incompatible ’',\n",
       " 'Get an error “<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>” after running an AWS CLI command',\n",
       " 'Pre-commit triggers an error at every commit: “mapping values are not allowed in this context”',\n",
       " 'Could not reconfigure pytest from zero after getting done with previous folder',\n",
       " 'Empty Records in Kinesis Get Records with LocalStack',\n",
       " 'In Powershell, Git commit raises utf-8 encoding error after creating pre-commit yaml file',\n",
       " \"Git commit with pre-commit hook raises error ‘'PythonInfo' object has no attribute 'version_nodot'\",\n",
       " 'Pytest error ‘module not found’ when if using custom packages in the source code',\n",
       " 'Pytest error ‘module not found’ when using pre-commit hooks if using custom packages in the source code',\n",
       " 'Github actions: Permission denied error when executing script file',\n",
       " 'Managing Multiple Docker Containers with docker-compose profile',\n",
       " 'AWS regions need to match docker-compose',\n",
       " 'Isort Pre-commit',\n",
       " 'How to destroy infrastructure created via GitHub Actions']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11bf83",
   "metadata": {},
   "source": [
    "## Q2. Vector search for question\n",
    "\n",
    "Now let's index these embeddings with minsearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e132fc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x72ec165e11f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7fb78",
   "metadata": {},
   "source": [
    "Evaluate this seach method. What's MRR for it?\n",
    "\n",
    "- 0.25\n",
    "- 0.35\n",
    "- 0.45\n",
    "- 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc01c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(query, course):\n",
    "\n",
    "\n",
    "    results = vindex.search(\n",
    "        query_vector=query,\n",
    "        filter_dict={'course': course},\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b696ab49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7540d415ddc647b19b5701daa3877919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q2_relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    query_vector = pipeline.transform([q['question']])\n",
    "    results = vector_search(query=query_vector, course=q['course'])\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    q2_relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe712e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3571284489590088"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr(q2_relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc78c7",
   "metadata": {},
   "source": [
    "## Q2 ANSWER\n",
    "b) 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503076dd",
   "metadata": {},
   "source": [
    "## Q3. Vector search for question and answer\n",
    "\n",
    "We only used question in Q2. We can use both question and answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58f74677",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "for doc in documents:\n",
    "    t = doc['question'] + ' ' + doc['text']\n",
    "    texts.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b3221e",
   "metadata": {},
   "source": [
    "Using the same pipeline (`min_df=3 for TF-IDF vectorizer and `n_components=128` for SVD), evaluate the performance of this\n",
    "approach\n",
    "\n",
    "What's the hitrate?\n",
    "\n",
    "- 0.62\n",
    "- 0.72\n",
    "- 0.82\n",
    "- 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96af6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d624ecbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x72ec177b82c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vindex = VectorSearch(keyword_fields={'course'})\n",
    "vindex.fit(X, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3715eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876b84979d874795ac566762241d31a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q3_relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    query_vector = pipeline.transform([q['question']])\n",
    "    results = vector_search(query=query_vector, course=q['course'])\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    q3_relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bc92a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8210503566025502"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_rate(q3_relevance_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b1efa",
   "metadata": {},
   "source": [
    "## Q3 ANSWER\n",
    "c) 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12074c02",
   "metadata": {},
   "source": [
    "## Q4. Qdrant\n",
    "\n",
    "Now let's evaluate the following settings in Qdrant:\n",
    "\n",
    "- `text = doc['question'] + ' ' + doc['text']`\n",
    "- `model_handle = \"jinaai/jina-embeddings-v2-small-en\"`\n",
    "- `limit = 5`\n",
    "\n",
    "What's the MRR?\n",
    "\n",
    "- 0.65\n",
    "- 0.75\n",
    "- 0.85\n",
    "- 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5184a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "client = QdrantClient(\"http://localhost:6333\") #connecting to local Qdrant instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5f47f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastembed in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from fastembed) (0.33.0)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from fastembed) (0.7.3)\n",
      "Requirement already satisfied: mmh3<6.0.0,>=4.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from fastembed) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.26 in /home/codespace/.local/lib/python3.12/site-packages (from fastembed) (2.2.4)\n",
      "Requirement already satisfied: onnxruntime!=1.20.0,>=1.17.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from fastembed) (1.22.0)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from fastembed) (11.1.0)\n",
      "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from fastembed) (0.1.5)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in /home/codespace/.local/lib/python3.12/site-packages (from fastembed) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from fastembed) (0.21.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from fastembed) (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (1.1.4)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/python/3.12.1/lib/python3.12/site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/python/3.12.1/lib/python3.12/site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /usr/local/python/3.12.1/lib/python3.12/site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (6.31.1)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (1.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0,>=2.31->fastembed) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0,>=2.31->fastembed) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0,>=2.31->fastembed) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0,>=2.31->fastembed) (2025.1.31)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fastembed --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dcbdc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import TextEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86c61fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"model\": \"BAAI/bge-small-zh-v1.5\",\n",
      "   \"sources\": {\n",
      "      \"hf\": \"Qdrant/bge-small-zh-v1.5\",\n",
      "      \"url\": \"https://storage.googleapis.com/qdrant-fastembed/fast-bge-small-zh-v1.5.tar.gz\",\n",
      "      \"_deprecated_tar_struct\": true\n",
      "   },\n",
      "   \"model_file\": \"model_optimized.onnx\",\n",
      "   \"description\": \"Text embeddings, Unimodal (text), Chinese, 512 input tokens truncation, Prefixes for queries/documents: not so necessary, 2023 year.\",\n",
      "   \"license\": \"mit\",\n",
      "   \"size_in_GB\": 0.09,\n",
      "   \"additional_files\": [],\n",
      "   \"dim\": 512,\n",
      "   \"tasks\": {}\n",
      "}\n",
      "{\n",
      "   \"model\": \"Qdrant/clip-ViT-B-32-text\",\n",
      "   \"sources\": {\n",
      "      \"hf\": \"Qdrant/clip-ViT-B-32-text\",\n",
      "      \"url\": null,\n",
      "      \"_deprecated_tar_struct\": false\n",
      "   },\n",
      "   \"model_file\": \"model.onnx\",\n",
      "   \"description\": \"Text embeddings, Multimodal (text&image), English, 77 input tokens truncation, Prefixes for queries/documents: not necessary, 2021 year\",\n",
      "   \"license\": \"mit\",\n",
      "   \"size_in_GB\": 0.25,\n",
      "   \"additional_files\": [],\n",
      "   \"dim\": 512,\n",
      "   \"tasks\": {}\n",
      "}\n",
      "{\n",
      "   \"model\": \"jinaai/jina-embeddings-v2-small-en\",\n",
      "   \"sources\": {\n",
      "      \"hf\": \"xenova/jina-embeddings-v2-small-en\",\n",
      "      \"url\": null,\n",
      "      \"_deprecated_tar_struct\": false\n",
      "   },\n",
      "   \"model_file\": \"onnx/model.onnx\",\n",
      "   \"description\": \"Text embeddings, Unimodal (text), English, 8192 input tokens truncation, Prefixes for queries/documents: not necessary, 2023 year.\",\n",
      "   \"license\": \"apache-2.0\",\n",
      "   \"size_in_GB\": 0.12,\n",
      "   \"additional_files\": [],\n",
      "   \"dim\": 512,\n",
      "   \"tasks\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "\n",
    "for model in TextEmbedding.list_supported_models():\n",
    "    if model[\"dim\"] == EMBEDDING_DIMENSIONALITY:\n",
    "        print(json.dumps(model, indent=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbb72a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "#model_handle = TextEmbedding(model_name=\"jinaai/jina-embeddings-v2-small-en\")\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8056f973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the collection name\n",
    "collection_name = \"homework3\"\n",
    "if client.collection_exists(collection_name):\n",
    "    client.delete_collection(collection_name)                               \n",
    "# Create the collection with specified vector parameters\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBEDDING_DIMENSIONALITY,  # Dimensionality of the vectors\n",
    "        distance=models.Distance.COSINE  # Distance metric for similarity search\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1d4cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "data-engineering-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "machine-learning-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n",
      "mlops-zoomcamp\n"
     ]
    }
   ],
   "source": [
    "points = []\n",
    "id = 0\n",
    "\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc['course'])\n",
    "    text = doc['question'] + ' ' + doc['text']\n",
    "    point = models.PointStruct(\n",
    "        id=id,\n",
    "        vector=models.Document(text=text, model=\"jinaai/jina-embeddings-v2-small-en\"), #embed text locally with \"jinaai/jina-embeddings-v2-small-en\" from FastEmbed\n",
    "        payload={\n",
    "            \"text\": doc['text'],\n",
    "            \"section\": doc['section'],\n",
    "            \"course\": doc['course'],\n",
    "            \"doc_id\": doc['id'],\n",
    "        } #save all needed metadata fields\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4406adae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=0, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2c87b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, limit=5):\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document( \n",
    "            text=query,\n",
    "            model=\"jinaai/jina-embeddings-v2-small-en\"\n",
    "        ),\n",
    "        limit=limit, # top closest matches\n",
    "        with_payload=True #to get metadata in the results\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ba7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50933d5682704a02a7198280d1c026b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c02e79ef\n",
      "points=[ScoredPoint(id=450, version=0, score=0.8800473, payload={'text': 'The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).', 'section': 'General course-related questions', 'course': 'machine-learning-zoomcamp', 'doc_id': '636f55d5'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=0, version=0, score=0.87771636, payload={'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\", 'section': 'General course-related questions', 'course': 'data-engineering-zoomcamp', 'doc_id': 'c02e79ef'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=441, version=0, score=0.84766006, payload={'text': 'Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)', 'section': 'General course-related questions', 'course': 'machine-learning-zoomcamp', 'doc_id': '67e2fd13'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=436, version=0, score=0.835594, payload={'text': 'The course videos are pre-recorded, you can start watching the course right now.\\nWe will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\\nYou can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.', 'section': 'General course-related questions', 'course': 'machine-learning-zoomcamp', 'doc_id': '39fda9f0'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=7, version=0, score=0.8296785, payload={'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.', 'section': 'General course-related questions', 'course': 'data-engineering-zoomcamp', 'doc_id': 'a482086d'}, vector=None, shard_key=None, order_value=None)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m results = search(query=q[\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m relevance = [\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdoc_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m == doc_id \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[32m      9\u001b[39m q4_relevance_total.append(relevance)\n",
      "\u001b[31mTypeError\u001b[39m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "q4_relevance_total = []\n",
    "\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    print(doc_id)\n",
    "    results = search(query=q['question'])\n",
    "    print(results)\n",
    "    relevance = [d.payload[\"doc_id\"] == doc_id for d in results.points]\n",
    "    q4_relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7407cd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'636f55d5'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.points[0].payload[\"doc_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130872a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54999669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
